{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四题：神经网络：三层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现内容：\n",
    "1. 实现一个三层感知机\n",
    "2. 对手写数字数据集进行分类\n",
    "3. 绘制损失值变化曲线\n",
    "4. 完成kaggle MNIST手写数字分类任务，根据给定的超参数训练模型，完成表格的填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这道题中，我们要实现一个三层感知机\n",
    "\n",
    "<img src=\"https://davidham3.github.io/blog/2018/09/11/logistic-regression/Fig2.png\" ,width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播\n",
    "\n",
    "我们实现一个最简单的三层感知机，一个输入层，一个隐藏层，一个输出层，隐藏层单元个数为$h$个，输出层有$K$个单元。\n",
    "\n",
    "1. 我们将第一层的输入，定义为$X \\in \\mathbb{R}^{n \\times m}$，n个样本，m个特征。  \n",
    "2. 输入层到隐藏层之间的权重(weight)与偏置(bias)，分别为$W_1 \\in \\mathbb{R}^{m \\times h}$，$b_1 \\in \\mathbb{R}^{1 \\times h}$。  \n",
    "3. 隐藏层到输出层的权重和偏置分为别$W_2 \\in \\mathbb{R}^{h \\times K}$，$b_2 \\in \\mathbb{R}^{1 \\times K}$。\n",
    "\n",
    "隐藏层的激活函数选用ReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(x) = \\max (0, x)\n",
    "$$\n",
    "\n",
    "我们用$H_1$表示第一个隐藏层的输出值，$O$表示输出层的输出值，这样，前向传播即可定义为\n",
    "\n",
    "$$\n",
    "Z = XW_1 + b_1\\\\\n",
    "H_1 = \\mathrm{ReLU}(Z)\\\\\n",
    "O = H_1 W_2 + b_2\n",
    "$$\n",
    "\n",
    "其中，$H_1 \\in \\mathbb{R}^{n \\times h}$，$O \\in \\mathbb{R}^{n \\times K}$。\n",
    "\n",
    "**注意：这里我们其实是做了广播，将$b_1$复制了$n-1$份后拼接成了维数为$n \\times h$的矩阵，同理，$b_2$也做了广播，拼成了$n \\times K$的矩阵。**\n",
    "\n",
    "最后一层的输出，使用softmax函数激活，得到神经网络计算出的各类的概率值：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y_i} & = \\mathrm{softmax}(O_i)\\\\\n",
    "& = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\hat{y_i}$表示第$i$类的概率值，也就是输出层第$i$个神经元经$\\mathrm{softmax}$激活后的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "损失函数使用交叉熵损失函数：\n",
    "$$\\mathrm{cross\\_entropy}(y, \\hat{y}) = -\\sum^{K}_{k=1}y_k \\log{(\\hat{y_k})}$$\n",
    "\n",
    "这样，$n$个样本的平均损失为：\n",
    "$$\n",
    "\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}\n",
    "$$\n",
    "\n",
    "**注意，这里我们的提到的$\\log$均为$\\ln$，在numpy中为**`np.log`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播\n",
    "\n",
    "我们使用梯度下降训练模型，求解方式就是求出损失函数对参数的偏导数，即参数的梯度，然后将参数减去梯度乘以学习率，进行参数的更新。\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "其中，$\\alpha$是学习率。\n",
    "\n",
    "在这道题中，交叉熵损失函数的求导比较麻烦，我们先求神经网络的输出层的偏导数，写成链式法则的形式：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O_i} = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O_i}\n",
    "$$\n",
    "\n",
    "首先求解第一项：\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\frac{1}{\\hat{y_k}}\n",
    "$$\n",
    "\n",
    "然后求解第二项，因为$\\hat{y_k}$的分母是$\\sum_k \\exp{(O_k)}$，里面包含$O_i$，所以每一个$\\hat{y_k}$的分母都包含$O_i$，这就要求反向传播的时候需要考虑这$K$项，将这$K$项的偏导数加在一起。\n",
    "\n",
    "这$K$项分别为：$\\frac{\\exp{(O_1)}}{\\sum_k \\exp{(O_k)}}$，$\\frac{\\exp{(O_2)}}{\\sum_k \\exp{(O_k)}}$，...，$\\frac{\\exp{(O_i)}}{\\sum_k \\exp{(O_k)}}$，...，$\\frac{\\exp{(O_k)}}{\\sum_k \\exp{(O_k)}}$。\n",
    "\n",
    "显然，这里只有分子带有$O_i$的这项与其他的项不同，因为分子和分母同时包含了$O_i$，而其他的项只有分母包含了$O_i$。\n",
    "\n",
    "这就需要在求解$\\frac{\\partial \\hat{y}}{\\partial O_i}$的时候分两种情况讨论\n",
    "1. 分子带$O_i$\n",
    "2. 分子不带$O_i$\n",
    "\n",
    "第一种情况，当分子含有$O_i$时：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat{y_i}}{\\partial O_i} & = \\frac{\\partial \\hat{y_i}}{\\partial O_i}\\\\\n",
    "& = \\frac{\\exp{(O_i)} (\\sum^{K}_{k=1} \\exp{(O_k)}) - (\\exp{(O_i)})^2 }{(\\sum^{K}_{k=1} \\exp{(O_k)})^2}\\\\\n",
    "& = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}} \\frac{\\sum^{K}_{k=1} \\exp{(O_k)} - \\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\\\\\n",
    "& = \\hat{y_i} ( 1 - \\hat{y_i} )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "第二种情况，当分子不含$O_i$时，我们用$j$表示当前项的下标：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\hat{y_j}}{\\partial O_i} & = \\frac{- \\exp{(O_j)} \\exp{(O_i)}}{(\\sum^{K}_{k=1} \\exp{(O_k)})^2}\\\\\n",
    "& = - \\hat{y_j} \\hat{y_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这样，$\\mathrm{loss}$对$O_i$的偏导数即为：\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O_i} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O_i}\\\\\n",
    "& = (- \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\frac{1}{\\hat{y_k}}) \\frac{\\partial \\hat{y}}{\\partial O_i}\\\\\n",
    "& = - \\frac{1}{n} \\sum_n (y_i \\frac{1}{\\hat{y_i}} \\hat{y_i} ( 1 - \\hat{y_i} ) + \\sum^K_{k \\not= i} y_k \\frac{1}{\\hat{y_k}}( - \\hat{y_k} \\hat{y_i}))\\\\\n",
    "& = - \\frac{1}{n} \\sum_n ( y_i - y_i \\hat{y_i} - \\sum^K_{k \\not= i} y_k \\hat{y_i})\\\\\n",
    "& = - \\frac{1}{n} \\sum_n ( y_i  - \\hat{y_i} \\sum^K_{k = 1} y_k )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由于我们处理的多类分类任务，一个样本只对应一个标记，所以$\\sum^K_{k = 1} y_k = 1$，上式在这种问题中，即可化简为：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O_i} &= - \\frac{1}{n} \\sum_n ( y_i  - \\hat{y_i})\\\\\n",
    "& = \\frac{1}{n} \\sum_n (\\hat{y_i} -  y_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "将其写成矩阵表达式：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial O} &= \\frac{1}{n} (\\hat{y} - y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "也就是说，我们的损失函数对输出层的$K$个神经单元的偏导数为$\\mathrm{softmax}$激活值减去真值。\n",
    "\n",
    "接下来我们需要求损失函数对参数$W_2$和$b_2$的偏导数\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial W_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial W_2}\\\\\n",
    "& = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial W_2}\\\\\n",
    "& = \\frac{1}{n} [{H_1}^\\mathrm{T} (\\hat{y} - y)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial b_2} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial b_2}\\\\\n",
    "& = \\frac{1}{n} (\\hat{y} - y) \\frac{\\partial O}{\\partial b_2}\\\\\n",
    "& = \\frac{1}{n} \\sum^n_{i=1} (\\hat{y_i} - y_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\frac{\\partial loss}{\\partial W_2} \\in \\mathbb{R}^{h \\times K}$，$\\frac{\\partial loss}{\\partial b_2} \\in \\mathbb{R}^{1 \\times K}$。  \n",
    "**注意，由于$b_2$是被广播成$n \\times K$的矩阵，因此实际上$b_2$对每个样本的损失都有贡献，因此对其求偏导时，要把$n$个样本对它的偏导数加和。**\n",
    "\n",
    "同理，我们可以求得$\\mathrm{loss}$对$W_1$和$b_1$的偏导数：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial W_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial W_1}\\\\\n",
    "& = \\frac{1}{n} {X}^\\mathrm{T} [(\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由于我们使用的是$\\mathrm{ReLU}$激活函数，它的偏导数为：\n",
    "\n",
    "$$\\frac{\\partial \\mathrm{ReLU(x)}}{\\partial x} = \n",
    "\\begin{cases}\n",
    "0 & \\text{if } x < 0\\\\\n",
    "1 & \\text{if } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "所以上式为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial {W_1}_{ij}} =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } {Z}_{ij} < 0\\\\\n",
    "    \\frac{1}{n} {X}^\\mathrm{T} (\\hat{y} - y) {W_2}^\\mathrm{T} & \\text{if } {Z}_{ij} \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "其中，${W_1}_{ij}$表示矩阵$W_1$第$i$行第$j$列的值，${Z}_{ij}$表示矩阵$Z$第$i$行第$j$列的值。  \n",
    "同理：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial loss}{\\partial b_1} & = \\frac{\\partial \\mathrm{loss}}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\\n",
    "& = \\frac{\\partial loss}{\\partial O} \\frac{\\partial O}{\\partial H_1} \\frac{\\partial H_1}{\\partial Z} \\frac{\\partial Z}{\\partial b_1}\\\\\n",
    "& = \\frac{1}{n} (\\hat{y} - y) {W_2}^\\mathrm{T} \\frac{\\partial H_1}{\\partial Z}\\\\\n",
    "& = \\begin{cases}\n",
    "0 &\\text{if } {Z}_{ij} < 0\\\\\n",
    "\\frac{1}{n} \\sum_n (\\hat{y} - y) {W_2}^\\mathrm{T} &\\text{if } {Z}_{ij} \\geq 0\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\frac{\\partial loss}{\\partial W_1} \\in \\mathbb{R}^{m \\times h}$，$\\frac{\\partial loss}{\\partial b_1} \\in \\mathbb{R}^{1 \\times h}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数更新\n",
    "\n",
    "求得损失函数对四个参数的偏导数后，我们就可以使用梯度下降进行参数更新：\n",
    "$$\n",
    "W_2 := W_2 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W_2}\\\\\n",
    "b_2 := b_2 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b_2}\\\\\n",
    "W_1 := W_1 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W_1}\\\\\n",
    "b_1 := b_1 - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b_1}\\\\\n",
    "$$\n",
    "其中，$\\alpha$是学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上内容，就是一个三层感知机的前向传播与反向传播过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用第一题的手写数字数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40%做测试集，60%做训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(load_digits()['data'], load_digits()['target'], test_size = 0.4, random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1078, 64), (1078,), (719, 64), (719,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用和第一题一样的标准化处理方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "trainX = s.fit_transform(trainX)\n",
    "testX = s.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来还要处理输出。  \n",
    "我们的神经网络是针对每个样本，输出其分别属于$K$类的概率，我们要找最大的那个概率，对应的是哪个类。  \n",
    "我们当前的trainY和testY，每个样本都是一个类标，我们需要将其变成one_hot编码，也就是，假设当前样本的类别是3，我们需要把它变成一个长度为10的向量，其中第4个元素为1，其他元素都为0。得到的矩阵分别记为trainY_mat和testY_mat。  \n",
    "这样，模型训练完成后，会针对每个样本输出十个数，分别代表这个样本属于$0,1,...,9$的概率，那我们只要取最大的那个数的下标，就知道模型认为这个样本是哪类了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_mat = np.zeros((len(trainY), 10))\n",
    "trainY_mat[np.arange(0, len(trainY), 1), trainY] = 1\n",
    "\n",
    "testY_mat = np.zeros((len(testY), 10))\n",
    "testY_mat[np.arange(0, len(testY), 1), testY] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1078, 10), (719, 10))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY_mat.shape, testY_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这题和上一题的区别是，我们把参数用dict存起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(h, K):\n",
    "    '''\n",
    "    参数初始化\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    h: int: 隐藏层单元个数\n",
    "    \n",
    "    K: int: 输出层单元个数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    parameters: dict，参数，键是\"W1\", \"b1\", \"W2\", \"b2\"\n",
    "    \n",
    "    '''\n",
    "    np.random.seed(32)\n",
    "    W_1 = np.random.normal(size = (trainX.shape[1], h)) * 0.01\n",
    "    b_1 = np.zeros((1, h))\n",
    "    \n",
    "    np.random.seed(32)\n",
    "    W_2 = np.random.normal(size = (h, K)) * 0.01\n",
    "    b_2 = np.zeros((1, K))\n",
    "    \n",
    "    parameters = {'W1': W_1, 'b1': b_1, 'W2': W_2, 'b2': b_2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50)\n",
      "(1, 50)\n",
      "(50, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "print(parameterst['W1'].shape) # (64, 50)\n",
    "print(parameterst['b1'].shape) # (1, 50)\n",
    "print(parameterst['W2'].shape) # (50, 10)\n",
    "print(parameterst['b2'].shape) # (1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成Z的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_combination(X, W, b):\n",
    "    '''\n",
    "    计算Z，Z = XW + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    W: np.ndarray, shape = (m, h)，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, h)，偏置\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Z: np.ndarray, shape = (n, h)，线性组合后的值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Z = XW + b\n",
    "    # YOUR CODE HERE\n",
    "    Z =       np.dot(X,W) + b              # YOUR CODE HERE\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 50)\n",
      "-5.273044421225233e-19\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "print(Zt.shape) # (1078, 50)\n",
    "print(Zt.mean()) # -5.27304442123e-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rm ReLU$激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    '''\n",
    "    ReLU激活函数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray，待激活的矩阵\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    activations: np.ndarray, 激活后的矩阵\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    activations = np.clip(X,0,X)\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030445670920609378\n",
      "(1078, 10)\n",
      "0.0006001926584638006\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "print(Ht.mean()) # 0.0304\n",
    "\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "print(Ot.shape) # (1078, 10)\n",
    "print(Ot.mean()) # 0.0006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rm softmax$激活  \n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(O_i) = \\frac{\\exp{(O_i)}}{\\sum^{K}_{k=1} \\exp{(O_k)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(O):\n",
    "    '''\n",
    "    softmax激活\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    softmax=np.exp(O)/np.sum(np.exp(O))\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]]\n",
      "[[nan nan nan]]\n",
      "[[nan nan nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "H:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 测试样例1\n",
    "\n",
    "print(my_softmax(np.array([[0.3, 0.3, 0.3]])))  # array([[ 0.33333333,  0.33333333,  0.33333333]])\n",
    "\n",
    "# 测试样例2\n",
    "test1 = np.array([[-1e32, -1e32, -1e32]])\n",
    "test2 = np.array([[1e32, 1e32, 1e32]])\n",
    "print(my_softmax(test1))\n",
    "print(my_softmax(test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，其实是有数值计算上的问题的，假设，我们最后的输出有三个数，每个数都特别小，理论上来说，通过$\\rm softmax$激活后，三个值都是$\\frac{1}{3}$。但实际上就不是这样了，实际上会导致分母为0，除法就不能做了。如果每个数都特别大，会导致做指数运算的时候上溢。\n",
    "\n",
    "我们需要用其他的方法来实现$\\rm softmax$。\n",
    "\n",
    "我们将传入$\\rm softmax$的向量，每个元素减去他们中的最大值，即\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(O_i) = \\mathrm{softmax}(O_i - \\mathrm{max(O)})\n",
    "$$\n",
    "\n",
    "这个式子是成立的，感兴趣的同学可以证明一下上面的式子。\n",
    "\n",
    "当我们做了这样的变换后，向量$O$中的最大值就变成了0，就不会上溢了，而分母中最少有一项为1，也不会出现下溢导致分母为0的问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(O):\n",
    "    '''\n",
    "    softmax激活函数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    O: np.ndarray，待激活的矩阵\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    activations: np.ndarray, 激活后的矩阵\n",
    "    \n",
    "    '''\n",
    "    print(O.shape)\n",
    "    # YOUR CODE HEER\n",
    "\n",
    "    activations =np.zeros(np.shape(O))\n",
    "    for i in range(np.shape(O)[0]):\n",
    "         activations[i]=  np.exp(O[i] - np.max(O))/np.sum(np.exp(O[i] - np.max(O)))\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 10)\n",
      "(1078, 10)\n",
      "0.0006001926584638006\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "y_pred = softmax(Ot)\n",
    "\n",
    "print(y_pred.shape)  # (1078, 10)\n",
    "print(Ot.mean())     # 0.000600192658464\n",
    "print(y_pred.mean()) # 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是实现损失函数，交叉熵损失函数：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss} = - \\frac{1}{n} \\sum_n \\sum^{K}_{k=1} y_k \\log{(\\hat{y_k})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里又会出一个问题，交叉熵损失函数中，我们需要对$\\rm softmax$的激活值取对数，也就是$\\log{\\hat{y}}$，这就要求我们的激活值全都是大于0的数，不能等于0，但是我们实现的$\\rm softmax$在有些时候确实会输出0，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(np.array([[1e32, 0, -1e32]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就使得在计算loss的时候会出现问题，解决这个问题的方法是$\\rm log \\ softmax$。所谓$\\rm log \\ softmax$，就是将交叉熵中的对数运算与$\\rm softmax$结合起来，避开为0的情况\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\log{\\frac{\\exp{(O_i)}}{\\sum_K \\exp{(O_k)}}} &= \\log{\\frac{\\exp{(O_i - \\mathrm{max}(O))}}{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}}\\\\\n",
    "&= O_i - \\mathrm{max}(O) - \\log{\\sum_K \\exp{(O_k - \\mathrm{max}(O))}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这样我们再计算$\\rm loss$的时候就可以把输出层的输出直接放到$\\rm log \\ softmax$中计算，不用先激活，再取对数了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先编写`log_softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    '''\n",
    "    log softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.ndarray，待激活的矩阵\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    log_activations: np.ndarray, 激活后取了对数的矩阵\n",
    "    \n",
    "    '''\n",
    "    log_activations =np.zeros(np.shape(x))\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        log_activations[i] =x[i]-np.max(x)-np.log(np.sum(np.exp(x[i]-np.max(x))))\n",
    "    return log_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 10)\n",
      "-2.3025914871652615\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "\n",
    "t = log_softmax(Ot)\n",
    "print(t.shape)  # (1078, 10)\n",
    "print(t.mean()) # -2.30259148717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后编写`cross_entropy_with_softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_with_softmax(y_true, O):\n",
    "    '''\n",
    "    求解交叉熵损失函数，这里需要使用log softmax，所以参数分别是真值和未经softmax激活的输出值\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray，shape = (n, K), 真值\n",
    "    \n",
    "    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    loss: float, 平均的交叉熵损失值\n",
    "    \n",
    "    '''\n",
    "    y_hat = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "    n=len(y_pred)\n",
    "    # 平均交叉熵损失\n",
    "    # YOUR CODE HERE\n",
    "    loss =  np.sum(y_true*log_softmax(O))/-n\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.302667079581974\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "losst = cross_entropy_with_softmax(trainY_mat, Ot)\n",
    "print(losst.mean()) # 2.30266707958"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正是因为$\\rm softmax$激活与交叉熵损失会有这样的问题，所以在很多深度学习框架中，交叉熵损失函数就直接带有了激活的功能，所以我们在实现前向传播计算的时候，就不要加$\\rm softmax$激活函数了。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, parameters):\n",
    "    '''\n",
    "    前向传播，从输入一直到输出层softmax激活前的值\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    O: np.ndarray, shape = (n, K)，softmax激活前的输出层的输出值\n",
    "    \n",
    "    '''\n",
    "    # 输入层到隐藏层\n",
    "    # YOUR CODE HERE\n",
    "    Z = np.dot(X,parameters['W1'])+parameters['b1']    \n",
    "    \n",
    "    # 隐藏层的激活\n",
    "    # YOUR CODE HERE\n",
    "    H = ReLU(Z)\n",
    "    \n",
    "    # 隐藏层到输出层\n",
    "    # YOUR CODE HERE\n",
    "    O = np.dot(H,parameters['W2'])+parameters['b2']\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006001926584638006\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "Ot = forward(trainX, parameterst)\n",
    "print(Ot.mean()) # 0.000600192658464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y_true, y_pred, H, Z, X, parameters):\n",
    "    '''\n",
    "    计算梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray，shape = (n, K), 真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n",
    "    \n",
    "    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n",
    "    \n",
    "    X: np.ndarray, shape = (n, m)，输入的原始数据\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    grads: dict, 梯度\n",
    "    \n",
    "    '''\n",
    "    n = np.shape(y_true)[0]\n",
    "\n",
    "    # 计算W2的梯度\n",
    "     # YOUR CODE HERE\n",
    "    dW2 = np.dot(H.T,(y_pred-y_true))/n\n",
    "    \n",
    "    # 计算b2的梯度\n",
    "    # YOUR CODE HERE\n",
    "    db2 = np.mean(y_pred-y_true,axis=0)\n",
    "    \n",
    "    # 计算ReLU的梯度\n",
    "    relu_grad = Z.copy()\n",
    "    relu_grad[relu_grad < 0] = 0\n",
    "    relu_grad[relu_grad >= 0] = 1\n",
    "    \n",
    "    # 计算W1的梯度\n",
    "    \n",
    "    db1tmp = np.dot((y_pred-y_true),parameters['W2'].T)*relu_grad/n\n",
    "    dW1 = np.dot(X.T,db1tmp)\n",
    "    db1 = np.mean(db1tmp,axis=0)\n",
    "    \n",
    "    grads = {'dW2': dW2, 'db2': db2, 'dW1': dW1, 'db1': db1}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 10)\n",
      "0.04291861176681737\n",
      "-4.693739813148599e-08\n",
      "-1.3444106938820255e-17\n",
      "-1.5178830414797062e-17\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "y_predt = softmax(Ot)\n",
    "\n",
    "gradst = compute_gradient(trainY_mat, y_predt, Ht, Zt, trainX, parameterst)\n",
    "\n",
    "print(gradst['dW1'].sum()) # 0.0429186117668\n",
    "print(gradst['db1'].sum()) # -5.05985151857e-05\n",
    "print(gradst['dW2'].sum()) # -2.16840434497e-18\n",
    "print(gradst['db2'].sum()) # -1.34441069388e-17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降，参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    参数更新\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters: dict，参数\n",
    "    \n",
    "    grads: dict, 梯度\n",
    "    \n",
    "    learning_rate: float, 学习率\n",
    "    \n",
    "    '''\n",
    "    parameters['W2'] -= learning_rate * grads['dW2']\n",
    "    parameters['b2'] -= learning_rate * grads['db2']\n",
    "    parameters['W1'] -= learning_rate * grads['dW1']\n",
    "    parameters['b1'] -= learning_rate * grads['db1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播，参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5834954544808615\n",
      "0.0\n",
      "0.18887164310031426\n",
      "0.0\n",
      "\n",
      "(1078, 10)\n",
      "0.57920359330418\n",
      "4.69373981314861e-09\n",
      "0.18887164310031426\n",
      "1.463672932855431e-18\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "print(parameterst['W1'].sum())  # 0.583495454481\n",
    "print(parameterst['b1'].sum())  # 0.0\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 0.0\n",
    "print()\n",
    "\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "y_predt = softmax(Ot)\n",
    "\n",
    "gradst = compute_gradient(trainY_mat, y_predt, Ht, Zt, trainX, parameterst)\n",
    "update(parameterst, gradst, 0.1)\n",
    "\n",
    "print(parameterst['W1'].sum())  # 0.579203593304\n",
    "print(parameterst['b1'].sum())  # 5.05985151857e-06\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 1.24683249836e-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(y_true, y_pred, H, Z, X, parameters, learning_rate):\n",
    "    '''\n",
    "    计算梯度，参数更新\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray，shape = (n, K), 真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, K)，softmax激活后的输出层的输出值\n",
    "    \n",
    "    H: np.ndarray, shape = (n, h)，隐藏层激活后的值\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, h), 隐藏层激活前的值\n",
    "    \n",
    "    X: np.ndarray, shape = (n, m)，输入的原始数据\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    learning_rate: float, 学习率\n",
    "    \n",
    "    '''\n",
    "    # 计算梯度\n",
    "    # YOUR CODE HERE\n",
    "    grads = compute_gradient(y_true, y_pred, H, Z, X, parameters)\n",
    "    \n",
    "    # 更新参数\n",
    "    update(parameters, grads, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5834954544808615\n",
      "0.0\n",
      "0.18887164310031426\n",
      "0.0\n",
      "\n",
      "(1078, 10)\n",
      "0.57920359330418\n",
      "4.69373981314861e-09\n",
      "0.18887164310031426\n",
      "1.463672932855431e-18\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "print(parameterst['W1'].sum())  # 0.583495454481\n",
    "print(parameterst['b1'].sum())  # 0.0\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 0.0\n",
    "print()\n",
    "\n",
    "Zt = linear_combination(trainX, parameterst['W1'], parameterst['b1'])\n",
    "Ht = ReLU(Zt)\n",
    "Ot = linear_combination(Ht, parameterst['W2'], parameterst['b2'])\n",
    "y_predt = softmax(Ot)\n",
    "\n",
    "backward(trainY_mat, y_predt, Ht, Zt, trainX, parameterst, 0.1)\n",
    "\n",
    "print(parameterst['W1'].sum())  # 0.579203593304\n",
    "print(parameterst['b1'].sum())  # 5.05985151857e-06\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 1.24683249836e-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainX, trainY, testX, testY, parameters, epochs, learning_rate = 0.01, verbose = False):\n",
    "    '''\n",
    "    训练\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    trainX: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    trainY: np.ndarray, shape = (n, K), 训练集标记\n",
    "    \n",
    "    testX: np.ndarray, shape = (n_test, m)，测试集\n",
    "    \n",
    "    testY: np.ndarray, shape = (n_test, K)，测试集的标记\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    epochs: int, 要迭代的轮数\n",
    "    \n",
    "    learning_rate: float, default 0.01，学习率\n",
    "    \n",
    "    verbose: boolean, default False，是否打印损失值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    # 存储损失值\n",
    "    training_loss_list = []\n",
    "    testing_loss_list = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # 这里要计算出Z和H，因为后面反向传播计算梯度的时候需要这两个矩阵\n",
    "        Z = linear_combination(trainX, parameters['W1'], parameters['b1'])\n",
    "        H = ReLU(Z)\n",
    "        train_O = linear_combination(H, parameters['W2'], parameters['b2'])\n",
    "        train_y_pred = softmax(train_O)\n",
    "        training_loss = cross_entropy_with_softmax(trainY, train_O)\n",
    "        \n",
    "        test_O = forward(testX, parameters)\n",
    "        testing_loss = cross_entropy_with_softmax(testY, test_O)\n",
    "        \n",
    "        if verbose == True:\n",
    "            print('epoch %s, training loss:%s'%(i + 1, training_loss))\n",
    "            print('epoch %s, testing loss:%s'%(i + 1, testing_loss))\n",
    "            print()\n",
    "        \n",
    "        training_loss_list.append(training_loss)\n",
    "        testing_loss_list.append(testing_loss)\n",
    "        \n",
    "        backward(trainY, train_y_pred, H, Z, trainX, parameters, learning_rate)\n",
    "    return training_loss_list, testing_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5834954544808615\n",
      "0.0\n",
      "0.18887164310031426\n",
      "0.0\n",
      "\n",
      "(1078, 10)\n",
      "0.57920359330418\n",
      "4.69373981314861e-09\n",
      "0.18887164310031426\n",
      "1.463672932855431e-18\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "parameterst = initialize(50, 10)\n",
    "print(parameterst['W1'].sum())  # 0.583495454481\n",
    "print(parameterst['b1'].sum())  # 0.0\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 0.0\n",
    "print()\n",
    "\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameterst, 1, 0.1, False)\n",
    "\n",
    "print(parameterst['W1'].sum())  # 0.579203593304\n",
    "print(parameterst['b1'].sum())  # 5.05985151857e-06\n",
    "print(parameterst['W2'].sum())  # 0.1888716431\n",
    "print(parameterst['b2'].sum())  # 1.24683249836e-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 绘制模型损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(training_loss_list, testing_loss_list):\n",
    "    '''\n",
    "    绘制损失值变化曲线\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(training_loss_list, label = 'training loss')\n",
    "    plt.plot(testing_loss_list, label = 'testing loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练完后，我们的就可以进行预测了，需要注意的是，我们的神经网络是针对每个样本，输出其分别属于$K$类的概率，我们要找最大的那个概率，对应的是哪个类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    '''\n",
    "    预测，调用forward函数完成神经网络对输入X的计算，然后完成类别的划分，取每行最大的那个数的下标作为标记\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    parameters: dict，参数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prediction: np.ndarray, shape = (n, 1)，预测的标记\n",
    "    \n",
    "    '''\n",
    "    # 用forward函数得到softmax激活前的值\n",
    "    O = forward(X, parameters)\n",
    "    \n",
    "    # 计算softmax激活后的值\n",
    "    y_pred = softmax(O)\n",
    "    \n",
    "    # 取每行最大的元素对应的下标\n",
    "    prediction = np.argmax(y_pred,axis=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 10)\n",
      "(719, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1599443671766342"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试样例\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "parameterst = initialize(50, 10)\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameterst, 1, 0.1, False)\n",
    "\n",
    "predictiont = predict(testX, parameterst)\n",
    "accuracy_score(predictiont, testY)  # 0.15994436717663421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 训练一个三层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐藏层单元数设置为50，输出层单元数为10，我们设置学习率为0.03，迭代轮数为1000轮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "(1078, 10)\n",
      "training time: 70.92465114593506 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "h = 50\n",
    "K = 10\n",
    "parameters = initialize(h, K)\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters, 1000, 0.03, False)\n",
    "\n",
    "end_time = time()\n",
    "print('training time: %s s'%(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算测试集精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(719, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9499304589707928"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict(testX, parameters)\n",
    "accuracy_score(prediction, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更换数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们换一个数据集，使用MNIST手写数字数据集。\n",
    "\n",
    "MNIST是最有名的手写数字数据集之一，主页：http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "MNIST手写数字数据集有60000个样本组成的训练集，10000个样本组成的测试集，是NIST的子集。数字的尺寸都是归一化后的，且都在图像的中央。可以从上方的主页下载。\n",
    "\n",
    "我们使用的数据集是kaggle手写数字识别比赛中的训练集。数据集一共42000行，785列，其中第1列是标记，第2列到第785列是图像从左上角到右下角的像素值。图像大小为28×28像素，单通道的灰度图像。\n",
    "\n",
    "我们使用的是kaggle提供的MNIST手写数字识别比赛的训练集。这个数据集还是手写数字的图片，只不过像素变成了 $28 \\times 28$，图片的尺寸变大了，而且数据集的样本量也大了。我们取30%为测试集，70%为训练集。训练集样本数有29400个，测试集12600个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/kaggle_mnist/mnist_train.csv')\n",
    "X = data.values[:, 1:].astype('float32')\n",
    "Y = data.values[:, 0]\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = 0.3, random_state = 32)\n",
    "\n",
    "trainY_mat = np.zeros((len(trainY), 10))\n",
    "trainY_mat[np.arange(0, len(trainY), 1), trainY] = 1\n",
    "\n",
    "testY_mat = np.zeros((len(testY), 10))\n",
    "testY_mat[np.arange(0, len(testY), 1), testY] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29400, 784), (29400,), (29400, 10), (12600, 784), (12600,), (12600, 10))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, trainY_mat.shape, testX.shape, testY.shape, testY_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制训练集前10个图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, figs = plt.subplots(1, 10, figsize=(8, 4))\n",
    "for f, img, lbl in zip(figs, trainX[:10], trainY[:10]):\n",
    "    f.imshow(img.reshape((28, 28)), cmap = 'gray')\n",
    "    f.set_title(lbl)\n",
    "    f.axes.get_xaxis().set_visible(False)\n",
    "    f.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test：请你使用kaggle MNIST数据集，根据下表设定各个超参数，计算测试集上的精度，绘制损失值变化曲线，填写下表\n",
    "\n",
    "任务流程：\n",
    "1. 对数据集进行标准化处理\n",
    "2. 设定学习率和迭代轮数进行训练\n",
    "3. 计算测试集精度\n",
    "4. 绘制曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 双击此处填写\n",
    "\n",
    "精度保留4位小数；训练时间单位为秒，保留两位小数。\n",
    "\n",
    "隐藏层单元数 | 学习率 | 迭代轮数 | 测试集精度 | 训练时间(秒)\n",
    "-|-|-|-\n",
    "100 | 0.1 | 50 |  | \n",
    "100 | 0.1 | 100 |  | \n",
    "100 | 0.1 | 150 |  | \n",
    "100 | 0.1 | 500 |  | \n",
    "100 | 0.01 | 500 |  | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(12600, 10)\n",
      "0.8073809523809524\n",
      "training time: 894.2214727401733 s\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n",
      "(29400, 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-230-dc9a4c635fd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mparameters1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtraining_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mpredictiont1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-213-7575baacdea7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(trainX, trainY, testX, testY, parameters, epochs, learning_rate, verbose)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mtrain_O\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_combination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mtrain_y_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_O\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mtraining_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_with_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_O\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-170-344a3760e379>\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(O)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m          \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   2318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[1;32m-> 2320\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   2321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# small reductions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "trainX = s.fit_transform(trainX)\n",
    "testX = s.transform(testX)\n",
    "trainY_mat = np.zeros((len(trainY), 10))\n",
    "trainY_mat[np.arange(0, len(trainY), 1), trainY] = 1\n",
    "\n",
    "testY_mat = np.zeros((len(testY), 10))\n",
    "testY_mat[np.arange(0, len(testY), 1), testY] = 1\n",
    "rate_list=[0.1,0.1,0.1,0.1,0.01]\n",
    "loop_list=[50,100,150,500,500]\n",
    "for i in range(5):\n",
    "    start_time = time()\n",
    "    parameters1 = initialize(100,10)\n",
    "    training_loss_list, testing_loss_list = train(trainX, trainY_mat, testX, testY_mat, parameters1, loop_list[i], rate_list[i], False)\n",
    "    end_time = time()\n",
    "    predictiont1 = predict(testX, parameters1)\n",
    "    print(accuracy_score(predictiont1, testY))\n",
    "    print('training time: %s s'%(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}